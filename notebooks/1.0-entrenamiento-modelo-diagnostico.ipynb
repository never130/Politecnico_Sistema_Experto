{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a42ae62",
   "metadata": {},
   "source": [
    "# 1.0 - Entrenamiento del Modelo de Diagnóstico Respiratorio\n",
    "\n",
    "**Objetivo:** Cargar el dataset sintético, preprocesarlo, entrenar un `DecisionTreeClassifier` y guardar el modelo entrenado para su uso en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddd710",
   "metadata": {},
   "source": [
    "## 1. Cargar Librerías y Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Rutas ---\n",
    "DATASET_PATH = '../data/processed/dataset_respiratorio.csv'\n",
    "MODEL_DIR = '../models/'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'diagnostico_dt_model.pkl')\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(\"Dataset cargado exitosamente.\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf8d80",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos\n",
    "\n",
    "El `DecisionTreeClassifier` de scikit-learn requiere que todas las características de entrada sean numéricas. Por lo tanto, debemos convertir las columnas categóricas (como `tos` o `fumador`) a un formato numérico. Usaremos `OneHotEncoder` para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características (X) y etiquetas (y)\n",
    "# Vamos a predecir tanto el diagnóstico como la gravedad.\n",
    "# En una implementación más avanzada, se podrían entrenar dos modelos separados.\n",
    "X = df.drop(['diagnostico', 'gravedad'], axis=1)\n",
    "y_diag = df['diagnostico']\n",
    "y_grav = df['gravedad']\n",
    "\n",
    "# Identificar columnas categóricas y numéricas\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Columnas Categóricas:\", categorical_features.tolist())\n",
    "print(\"Columnas Numéricas:\", numerical_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el transformador de preprocesamiento\n",
    "# OneHotEncoder para las categóricas, y 'passthrough' para las numéricas (ya están listas)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ff4c7",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del Modelo de Árbol de Decisión\n",
    "\n",
    "Entrenaremos un modelo para predecir el **diagnóstico**. Podríamos entrenar otro para la gravedad si fuera necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_diag, test_size=0.2, random_state=42, stratify=y_diag)\n",
    "\n",
    "# Crear el pipeline del modelo\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', DecisionTreeClassifier(max_depth=5, random_state=42))])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Modelo entrenado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8933af",
   "metadata": {},
   "source": [
    "## 4. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f51b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\n",
    "Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\n",
    "Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_pred), index=model_pipeline.classes_, columns=model_pipeline.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb41f9",
   "metadata": {},
   "source": [
    "## 5. Guardar el Modelo Entrenado\n",
    "\n",
    "Finalmente, guardamos el pipeline completo (preprocesador + clasificador) en un archivo `.pkl` para que pueda ser cargado y utilizado por nuestra aplicación Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el pipeline del modelo\n",
    "joblib.dump(model_pipeline, MODEL_PATH)\n",
    "\n",
    "print(f\"Modelo guardado en: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
